{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinAI Contest Task 2 – LoRA Fine-Tuning Walk-through\n",
    "\n",
    "This notebook walks through **data preparation, fine-tuning, and inference** for FinAI Task 2 using the [FinLoRA](https://github.com/Open-Finance-Lab/FinLoRA) framework. It is simplified. For more detailed instructions, please check the tutorials under the FinLoRA docs here: https://finlora-docs.readthedocs.io/en/latest/index.html. The full process for a **simplified** Buffett Agent model we created can be found here: https://finlora-docs.readthedocs.io/en/latest/tutorials/buffett_agent.html.\n",
    "\n",
    "**Target Tasks:**\n",
    "- CFA exams\n",
    "- BloombergGPT public benchmarks  \n",
    "- XBRL tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "- NVIDIA GPU with ≥ 24 GB VRAM (8-bit) or ≥ 16 GB VRAM (4-bit)\n",
    "- CUDA ≥ 11.8\n",
    "- Alternatively, use runpod.io (see FinLoRA docs for instructions on using it: https://finlora-docs.readthedocs.io/en/latest/tutorials/setup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone FinLoRA and install dependencies\n",
    "!git clone https://github.com/Open-Finance-Lab/FinLoRA.git\n",
    "%cd FinLoRA\n",
    "\n",
    "# Option A - bash script\n",
    "!chmod +x setup.sh && ./setup.sh\n",
    "\n",
    "# Option B - conda (alternative)\n",
    "# !conda env create -f environment.yml && conda activate finenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate for gated Llama models\n",
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "**Data Sources to Collect:**\n",
    "- CFA mock-exam PDFs or CSVs\n",
    "- BloombergGPT benchmark datasets (FPB, FiQA SA, Headline, NER, ConvFinQA)\n",
    "- XBRL corpora for tag/value/formula tasks\n",
    "\n",
    "**Required Format:** JSONL with `{\"context\": \"<question>\", \"target\": \"<answer>\"}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume you have collected raw Q&A pairs in 'finai_raw.jsonl'\n",
    "# Each line should be: {\"context\": \"question\", \"target\": \"answer\"}\n",
    "# If you want to test your adapter on a test set, you can split the data into trai and test sets as follows.\n",
    "\n",
    "# Read raw data\n",
    "raw_file = Path('data/finai_raw.jsonl')  # Update path as needed\n",
    "if raw_file.exists():\n",
    "    with open(raw_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    # Shuffle for random split\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # 80/20 split\n",
    "    n = len(lines)\n",
    "    n_train = int(0.8 * n)\n",
    "    \n",
    "    train_lines = lines[:n_train]\n",
    "    test_lines = lines[n_train:]\n",
    "    \n",
    "    # Create directories\n",
    "    Path('data/train').mkdir(parents=True, exist_ok=True)\n",
    "    Path('data/test').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save splits\n",
    "    with open('data/train/finai_train.jsonl', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_lines) + '\\n')\n",
    "    \n",
    "    \n",
    "    with open('data/test/finai_test.jsonl', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(test_lines) + '\\n')\n",
    "    \n",
    "    print(f\"Split {n} examples into {len(train_lines)} examples for fine-tuning and {len(test_lines)} examples for testing\")\n",
    "else:\n",
    "    print(f\"Please create {raw_file} with your collected Q&A pairs first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Fine-Tuning\n",
    "\n",
    "Add configuration to `finetune_configs.json` for your FinAI model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read existing config\n",
    "config_file = Path('lora/finetune_configs.json')\n",
    "with open(config_file, 'r') as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "# Add competition fine-tuned model configuration\n",
    "configs[\"finai_llama_3_1_8b_8bits_r8_lora\"] = {\n",
    "    \"base_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"dataset_path\": \"../data/train/competition_train.jsonl\",\n",
    "    \"lora_r\": 8,\n",
    "    \"quant_bits\": 8,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 4,\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2\n",
    "}\n",
    "\n",
    "# Save updated config\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(configs, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning\n",
    "\n",
    "This will take some time depending on your dataset size and GPU setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch DeepSpeed configs and run fine-tuning\n",
    "%cd lora\n",
    "!axolotl fetch deepspeed_configs\n",
    "\n",
    "# Run the fine-tuning\n",
    "!python finetune.py competition_llama_3_1_8b_8bits_r8_lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Adapter & Run Inference\n",
    "\n",
    "Once fine-tuning is complete, you can run inferences as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# Path to your adapter\n",
    "adapter_path = \"axolotl-output/competitionllama_3_1_8b_8bits_r8_lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load and apply the LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is the primary purpose of a cash flow hedge under IFRS?\",\n",
    "    \"Explain the concept of economic value added (EVA).\",\n",
    "    \"How do you calculate the price-to-earnings ratio?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Answer: {response[len(question):].strip()}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
